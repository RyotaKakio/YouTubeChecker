import os
import streamlit as st
from googleapiclient.discovery import build
from urllib.parse import urlparse, parse_qs
from dotenv import load_dotenv
from youtube_transcript_api import YouTubeTranscriptApi
import whisper
from pytube import YouTube
import tempfile
import cv2
import numpy as np
from PIL import Image
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image
import yt_dlp

# Load environment variables
load_dotenv()
API_KEY = os.getenv('YOUTUBE_API_KEY')

if not API_KEY:
    st.error('APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚')
    st.stop()

youtube = build('youtube', 'v3', developerKey=API_KEY, static_discovery=False)

# ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹åŸºæº–ã®å®šç¾©
COMPLIANCE_RULES = {
    'ä¸€èˆ¬å€«ç†åŸºæº–': {
        'å·®åˆ¥çš„è¡¨ç¾': [
            'å·®åˆ¥', 'ä¾®è”‘', 'é¦¬é¹¿', 'ãƒã‚«', 'ã‚¢ãƒ›', 'æ­»ã­', 'ãã‚‚ã„',
            'åœ¨æ—¥', 'é»’äºº', 'ç™½äºº', 'ãƒ›ãƒ¢', 'ãƒ¬ã‚º', 'éšœå®³è€…', 'ç—…äºº',
            'ãƒåº¦', 'çŸ¥æµé…ã‚Œ', 'æ°—é•ã„', 'åŸºåœ°å¤–'
        ],
        'æ€§åˆ¥å·®åˆ¥': [
            'å¥³ã®ãã›ã«', 'ç”·ã®ãã›ã«', 'å¥³ã ã‹ã‚‰', 'ç”·ã ã‹ã‚‰',
            'å¥³ã®åˆ†éš›', 'ç”·ã®åˆ†éš›', 'å¥³ã‚‰ã—ã', 'ç”·ã‚‰ã—ã'
        ],
        'å®—æ•™é–¢é€£': [
            'ä»æ•™å¾’', 'ã‚­ãƒªã‚¹ãƒˆæ•™å¾’', 'ã‚¤ã‚¹ãƒ©ãƒ æ•™å¾’', 'å‰µä¾¡',
            'çµ±ä¸€æ•™ä¼š', 'å¹¸ç¦ã®ç§‘å­¦'
        ],
        'ä¸è¬¹æ…ãªè¡¨ç¾': [
            'éœ‡ç½', 'æ´¥æ³¢', 'å°é¢¨', 'ç½å®³', 'ã‚³ãƒ­ãƒŠ', 'ãƒ‘ãƒ³ãƒ‡ãƒŸãƒƒã‚¯',
            'äº‹æ•…', 'è‡ªæ®º', 'æ­»äº¡', 'æ®ºäºº'
        ]
    },
    'æ³•å¾‹åŸºæº–': {
        'æ™¯è¡¨æ³•é–¢é€£': [
            'æœ€é«˜', 'æœ€ä½', 'æœ€å¤§', 'æœ€å°', 'æœ€é«˜ç´š', 'æ¥­ç•Œä¸€', 'æ—¥æœ¬ä¸€',
            'ä¸–ç•Œä¸€', 'æœ€å®‰å€¤', 'å®Œå…¨', 'å®Œç’§', 'æ°¸ä¹…', 'ç¢ºå®Ÿ', 'ä¿è¨¼',
            'é–“é•ã„ãªã„', 'å¿…ãš'
        ],
        'è–¬æ©Ÿæ³•é–¢é€£': [
            'æ²»ç™‚', 'æ²»ç™’', 'å®Œæ²»', 'äºˆé˜²', 'åŠ¹èƒ½', 'åŠ¹æœ', 'å‰¯ä½œç”¨',
            'åŒ»è–¬å“', 'åŒ»ç™‚æ©Ÿå™¨', 'å‡¦æ–¹', 'æŠ•è–¬', 'æœç”¨'
        ],
        'åŠ´åƒæ³•é–¢é€£': [
            'æ®‹æ¥­ä»£', 'è³ƒé‡‘', 'çµ¦ä¸', 'æœªæ‰•ã„', 'é•·æ™‚é–“åŠ´åƒ', 
            'ã‚µãƒ¼ãƒ“ã‚¹æ®‹æ¥­', 'ä¼‘æ†©ãªã—', 'æœ‰çµ¦ä¼‘æš‡', 'åŠ´ç½'
        ]
    },
    'è‘—ä½œæ¨©ãƒ»è‚–åƒæ¨©': {
        'è‘—ä½œæ¨©ä¾µå®³': [
            'ç„¡æ–­ä½¿ç”¨', 'ç„¡æ–­è»¢è¼‰', 'ç„¡è¨±å¯', 'ã‚³ãƒ”ãƒ¼', 'è‘—ä½œæ¨©',
            'è»¢è¼‰', 'è¤‡è£½', 'æµ·è³Šç‰ˆ', 'é•æ³•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰'
        ],
        'è‚–åƒæ¨©ä¾µå®³': [
            'å€‹äººæƒ…å ±', 'ä½æ‰€', 'é›»è©±ç•ªå·', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',
            'é¡”å†™çœŸ', 'èº«åˆ†è¨¼', 'å…è¨±è¨¼', 'ãƒ‘ã‚¹ãƒãƒ¼ãƒˆ'
        ]
    },
    'ä¼æ¥­ç§˜å¯†': {
        'æ©Ÿå¯†æƒ…å ±': [
            'æ©Ÿå¯†', 'æ¥µç§˜', 'ç¤¾å¤–ç§˜', 'éƒ¨å¤–ç§˜', 'é–¢ä¿‚è€…å¤–', 
            'æœªç™ºè¡¨', 'æœªå…¬é–‹', 'å†…éƒ¨æƒ…å ±', 'ã‚¤ãƒ³ã‚µã‚¤ãƒ€ãƒ¼',
            'æ–°è£½å“', 'é–‹ç™ºä¸­', 'è©¦ä½œå“', 'ä¼ç”»æ›¸', 'è¨­è¨ˆå›³','å£²ä¸Š','åˆ©ç›Š'
        ]
    }
}

# è¡¨è¨˜ã‚†ã‚Œå¯¾å¿œã®å®šç¾©
KEYWORD_VARIATIONS = {
    'å·®åˆ¥': ['ã•ã¹ã¤', 'ã‚µãƒ™ãƒ„'],
    'ä¾®è”‘': ['ã¶ã¹ã¤', 'ãƒã‚«ã«ã™ã‚‹'],
    'é¦¬é¹¿': ['ãƒã‚«', 'ã‚¢ãƒ›', 'ã‚ã»', 'é˜¿å‘†', 'ã‚¢ãƒ•ã‚©'],
    'æ­»ã­': ['ã—ã­', 'ããŸã°ã‚Œ', 'ã€‡ã­'],
    'ãã‚‚ã„': ['ã‚­ãƒ¢ã‚¤', 'ãã‚‚ã£'],
    'éšœå®³è€…': ['ã—ã‚‡ã†ãŒã„ã—ã‚ƒ', 'ã‚·ãƒ§ã‚¦ã‚¬ã‚¤ã‚·ãƒ£', 'éšœãŒã„è€…'],
    'ç—…äºº': ['ã³ã‚‡ã†ã«ã‚“', 'ãƒ“ãƒ§ã‚¦ãƒ‹ãƒ³'],
    'çŸ¥æµé…ã‚Œ': ['ã¡ãˆãŠãã‚Œ', 'ãƒã‚¨ã‚ªã‚¯ãƒ¬'],
    'æ°—é•ã„': ['ãã¡ãŒã„', 'ã‚­ãƒã‚¬ã‚¤'],
    'åŸºåœ°å¤–': ['ãã¡ãŒã„', 'ã‚­ãƒã‚¬ã‚¤'],
    'å¥³ã®ãã›ã«': ['ãŠã‚“ãªã®ãã›ã«', 'ã‚ªãƒ³ãƒŠã®ãã›ã«'],
    'ç”·ã®ãã›ã«': ['ãŠã¨ã“ã®ãã›ã«', 'ã‚ªãƒˆã‚³ã®ãã›ã«'],
    'å¥³ã ã‹ã‚‰': ['ãŠã‚“ãªã ã‹ã‚‰', 'ã‚ªãƒ³ãƒŠã ã‹ã‚‰'],
    'ç”·ã ã‹ã‚‰': ['ãŠã¨ã“ã ã‹ã‚‰', 'ã‚ªãƒˆã‚³ã ã‹ã‚‰'],
    'å¥³ã®åˆ†éš›': ['ãŠã‚“ãªã®ã¶ã‚“ã–ã„', 'ã‚ªãƒ³ãƒŠã®ãƒ–ãƒ³ã‚¶ã‚¤'],
    'ç”·ã®åˆ†éš›': ['ãŠã¨ã“ã®ã¶ã‚“ã–ã„', 'ã‚ªãƒˆã‚³ã®ãƒ–ãƒ³ã‚¶ã‚¤'],
    'ä»æ•™å¾’': ['ã¶ã£ãã‚‡ã†ã¨', 'ãƒ–ãƒƒã‚­ãƒ§ã‚¦ãƒˆ'],
    'ã‚­ãƒªã‚¹ãƒˆæ•™å¾’': ['ã‚¯ãƒªã‚¹ãƒãƒ£ãƒ³', 'ãã‚Šã™ã¡ã‚ƒã‚“'],
    'ã‚¤ã‚¹ãƒ©ãƒ æ•™å¾’': ['ãƒ ã‚¹ãƒªãƒ ', 'ã‚€ã™ã‚Šã‚€'],
    'éœ‡ç½': ['ã—ã‚“ã•ã„', 'ã‚·ãƒ³ã‚µã‚¤'],
    'æ´¥æ³¢': ['ã¤ãªã¿', 'ãƒ„ãƒŠãƒŸ'],
    'å°é¢¨': ['ãŸã„ãµã†', 'ã‚¿ã‚¤ãƒ•ã‚¦'],
    'ç½å®³': ['ã•ã„ãŒã„', 'ã‚µã‚¤ã‚¬ã‚¤'],
    'ã‚³ãƒ­ãƒŠ': ['corona', 'Corona', 'COVID'],
    'ãƒ‘ãƒ³ãƒ‡ãƒŸãƒƒã‚¯': ['pandemic', 'ã±ã‚“ã§ã¿ã£ã'],
    'è‡ªæ®º': ['ã˜ã•ã¤', 'ã‚¸ã‚µãƒ„'],
    'æ­»äº¡': ['ã—ã¼ã†', 'ã‚·ãƒœã‚¦'],
    'æ®ºäºº': ['ã•ã¤ã˜ã‚“', 'ã‚µãƒ„ã‚¸ãƒ³'],
    'æœ€é«˜': ['ã•ã„ã“ã†', 'ã‚µã‚¤ã‚³ã‚¦', 'ã‚µã‚¤ã‚³ãƒ¼'],
    'æœ€ä½': ['ã•ã„ã¦ã„', 'ã‚µã‚¤ãƒ†ã‚¤'],
    'æœ€å¤§': ['ã•ã„ã ã„', 'ã‚µã‚¤ãƒ€ã‚¤'],
    'æœ€å°': ['ã•ã„ã—ã‚‡ã†', 'ã‚µã‚¤ã‚·ãƒ§ã‚¦'],
    'æœ€é«˜ç´š': ['ã•ã„ã“ã†ãã‚…ã†', 'ã‚µã‚¤ã‚³ã‚¦ã‚­ãƒ¥ã‚¦'],
    'æ¥­ç•Œä¸€': ['ãã‚‡ã†ã‹ã„ã„ã¡', 'ã‚®ãƒ§ã‚¦ã‚«ã‚¤ã‚¤ãƒ'],
    'å®Œå…¨': ['ã‹ã‚“ãœã‚“', 'ã‚«ãƒ³ã‚¼ãƒ³'],
    'å®Œç’§': ['ã‹ã‚“ãºã', 'ã‚«ãƒ³ãƒšã‚­'],
    'æ°¸ä¹…': ['ãˆã„ãã‚…ã†', 'ã‚¨ã‚¤ã‚­ãƒ¥ã‚¦'],
    'ç¢ºå®Ÿ': ['ã‹ãã˜ã¤', 'ã‚«ã‚¯ã‚¸ãƒ„'],
    'æ²»ç™‚': ['ã¡ã‚Šã‚‡ã†', 'ãƒãƒªãƒ§ã‚¦'],
    'æ²»ç™’': ['ã¡ã‚†', 'ãƒãƒ¦'],
    'å®Œæ²»': ['ã‹ã‚“ã¡', 'ã‚«ãƒ³ãƒ'],
    'äºˆé˜²': ['ã‚ˆã¼ã†', 'ãƒ¨ãƒœã‚¦'],
    'åŠ¹èƒ½': ['ã“ã†ã®ã†', 'ã‚³ã‚¦ãƒã‚¦'],
    'åŠ¹æœ': ['ã“ã†ã‹', 'ã‚³ã‚¦ã‚«'],
    'å‰¯ä½œç”¨': ['ãµãã•ã‚ˆã†', 'ãƒ•ã‚¯ã‚µãƒ¨ã‚¦'],
    'åŒ»è–¬å“': ['ã„ã‚„ãã²ã‚“', 'ã‚¤ãƒ¤ã‚¯ãƒ’ãƒ³'],
    'åŒ»ç™‚æ©Ÿå™¨': ['ã„ã‚Šã‚‡ã†ãã', 'ã‚¤ãƒªãƒ§ã‚¦ã‚­ã‚­'],
    'æ®‹æ¥­ä»£': ['ã–ã‚“ãã‚‡ã†ã ã„', 'ã‚¶ãƒ³ã‚®ãƒ§ã‚¦ãƒ€ã‚¤'],
    'è³ƒé‡‘': ['ã¡ã‚“ãã‚“', 'ãƒãƒ³ã‚®ãƒ³'],
    'çµ¦ä¸': ['ãã‚…ã†ã‚ˆ', 'ã‚­ãƒ¥ã‚¦ãƒ¨'],
    'æœªæ‰•ã„': ['ã¿ã°ã‚‰ã„', 'ãƒŸãƒãƒ©ã‚¤'],
    'é•·æ™‚é–“åŠ´åƒ': ['ã¡ã‚‡ã†ã˜ã‹ã‚“ã‚ã†ã©ã†', 'ãƒãƒ§ã‚¦ã‚¸ã‚«ãƒ³ãƒ­ã‚¦ãƒ‰ã‚¦'],
    'æœ‰çµ¦ä¼‘æš‡': ['ã‚†ã†ãã‚…ã†', 'ãƒ¦ã‚¦ã‚­ãƒ¥ã‚¦'],
    'åŠ´ç½': ['ã‚ã†ã•ã„', 'ãƒ­ã‚¦ã‚µã‚¤'],
    'ç„¡æ–­ä½¿ç”¨': ['ã‚€ã ã‚“ã—ã‚ˆã†', 'ãƒ ãƒ€ãƒ³ã‚·ãƒ¨ã‚¦'],
    'ç„¡æ–­è»¢è¼‰': ['ã‚€ã ã‚“ã¦ã‚“ã•ã„', 'ãƒ ãƒ€ãƒ³ãƒ†ãƒ³ã‚µã‚¤'],
    'ç„¡è¨±å¯': ['ã‚€ãã‚‡ã‹', 'ãƒ ã‚­ãƒ§ã‚«'],
    'è‘—ä½œæ¨©': ['ã¡ã‚‡ã•ãã‘ã‚“', 'ãƒãƒ§ã‚µã‚¯ã‚±ãƒ³'],
    'è¤‡è£½': ['ãµãã›ã„', 'ãƒ•ã‚¯ã‚»ã‚¤'],
    'æ©Ÿå¯†': ['ãã¿ã¤', 'ã‚­ãƒŸãƒ„'],
    'æ¥µç§˜': ['ã”ãã²', 'ã‚´ã‚¯ãƒ’'],
    'ç¤¾å¤–ç§˜': ['ã—ã‚ƒãŒã„ã²', 'ã‚·ãƒ£ã‚¬ã‚¤ãƒ’'],
    'éƒ¨å¤–ç§˜': ['ã¶ãŒã„ã²', 'ãƒ–ã‚¬ã‚¤ãƒ’'],
    'ã‚¤ãƒ³ã‚µã‚¤ãƒ€ãƒ¼': ['insider', 'Insider'],
    'æ–°è£½å“': ['ã—ã‚“ã›ã„ã²ã‚“', 'ã‚·ãƒ³ã‚»ã‚¤ãƒ’ãƒ³'],
    'é–‹ç™ºä¸­': ['ã‹ã„ã¯ã¤ã¡ã‚…ã†', 'ã‚«ã‚¤ãƒãƒ„ãƒãƒ¥ã‚¦'],
    'è©¦ä½œå“': ['ã—ã•ãã²ã‚“', 'ã‚·ã‚µã‚¯ãƒ’ãƒ³'],
    'ä¼ç”»æ›¸': ['ãã‹ãã—ã‚‡', 'ã‚­ã‚«ã‚¯ã‚·ãƒ§'],
    'è¨­è¨ˆå›³': ['ã›ã£ã‘ã„ãš', 'ã‚»ãƒƒã‚±ã‚¤ã‚º'],
    'å£²ä¸Š': ['ã†ã‚Šã‚ã’', 'ã‚¦ãƒªã‚¢ã‚²'],
    'åˆ©ç›Š': ['ã‚Šãˆã', 'ãƒªã‚¨ã‚­']
}

def normalize_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–ï¼ˆè¡¨è¨˜ã‚†ã‚Œå¯¾å¿œï¼‰"""
    normalized_text = text.lower()
    for standard, variations in KEYWORD_VARIATIONS.items():
        for variant in [standard] + variations:
            normalized_text = normalized_text.replace(variant.lower(), standard.lower())
    return normalized_text

def extract_video_id(url):
    """URLã‹ã‚‰ãƒ“ãƒ‡ã‚ªIDã‚’æŠ½å‡º"""
    parsed_url = urlparse(url)
    if 'youtube.com' in parsed_url.netloc:
        return parse_qs(parsed_url.query).get('v', [None])[0]
    elif 'youtu.be' in parsed_url.netloc:
        return parsed_url.path[1:]
    return None

def check_inappropriate_content(text):
    """ä¸é©åˆ‡ãªè¡¨ç¾ã®ãƒã‚§ãƒƒã‚¯"""
    results = []
    normalized_text = normalize_text(text)
    
    for main_category, subcategories in COMPLIANCE_RULES.items():
        category_results = []
        
        for sub_category, keywords in subcategories.items():
            for keyword in keywords:
                if normalize_text(keyword) in normalized_text:
                    category_results.append(f"ãƒ»{sub_category}: '{keyword}'ã‚’å«ã‚€è¡¨ç¾ãŒã‚ã‚Šã¾ã™")
        
        if category_results:
            results.append(f"\nã€{main_category}ã€‘")
            results.extend(category_results)
    
    return results

def analyze_video_content(video_data):
    """å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç·åˆåˆ†æ"""
    return {
        'title': check_inappropriate_content(video_data['snippet']['title']),
        'description': check_inappropriate_content(video_data['snippet']['description']),
        'tags': check_inappropriate_content(' '.join(video_data['snippet'].get('tags', [])))
    }

def display_analysis_results(analysis_results):
    """åˆ†æçµæœã®è©³ç´°è¡¨ç¤º"""
    st.write("ãƒã‚§ãƒƒã‚¯é …ç›®ã¨æ¤œæŸ»ãƒ—ãƒ­ã‚»ã‚¹:")
    
    # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒã‚§ãƒƒã‚¯å†…å®¹ã‚’è¡¨ç¤º
    for main_category, subcategories in COMPLIANCE_RULES.items():
        with st.expander(f"ğŸ“‹ {main_category}ã®ãƒã‚§ãƒƒã‚¯é …ç›®"):
            for sub_category, keywords in subcategories.items():
                st.write(f"âœ“ {sub_category}:")
                st.write(f"  ç¢ºèªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}")
    
    st.write("---")
    st.write("åˆ†æçµæœ:")
    
    if any(analysis_results.values()):
        st.error('ä»¥ä¸‹ã®æ³¨æ„ç‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š')
        
        sections = {
            'title': 'ã‚¿ã‚¤ãƒˆãƒ«',
            'description': 'èª¬æ˜æ–‡',
            'tags': 'ã‚¿ã‚°'
        }
        
        for section, name in sections.items():
            if analysis_results[section]:
                st.warning(f'â–  {name}ã§ã®æ³¨æ„ç‚¹ï¼š')
                for result in analysis_results[section]:
                    st.write(result)
            else:
                st.success(f'âœ“ {name}ã¯å•é¡Œãªã—')
    else:
        st.success('ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ä¸Šã®é‡å¤§ãªå•é¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')
        st.write("åˆ¤å®šæ ¹æ‹ :")
        for section in ['ã‚¿ã‚¤ãƒˆãƒ«', 'èª¬æ˜æ–‡', 'ã‚¿ã‚°']:
            st.write(f"âœ“ {section}ï¼šã™ã¹ã¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ã§è©²å½“ãªã—")

def get_video_transcription(video_id):
    """å‹•ç”»ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ™‚é–“æƒ…å ±ä»˜ãã§å–å¾—"""
    try:
        st.write("å­—å¹•/éŸ³å£°ã®è§£æã‚’é–‹å§‹...")
        
        try:
            st.write("å­—å¹•ã®å–å¾—ã‚’è©¦ã¿ã¦ã„ã¾ã™...")
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ja', 'en'])
            st.success("âœ… å­—å¹•ã®å–å¾—ã«æˆåŠŸã—ã¾ã—ãŸ")
            
            # æ™‚é–“æƒ…å ±ä»˜ãã§å­—å¹•ã‚’ä¿æŒ
            formatted_transcript = []
            for entry in transcript:
                time_in_minutes = int(entry['start']) // 60
                time_in_seconds = int(entry['start']) % 60
                formatted_transcript.append({
                    'text': entry['text'],
                    'time': f"{time_in_minutes}:{time_in_seconds:02d}",
                    'start': entry['start']
                })
            
            return {
                'text': " ".join([t['text'] for t in formatted_transcript]),
                'source': 'å­—å¹•',
                'segments': formatted_transcript
            }
            
        except Exception as e:
            st.warning(f"å­—å¹•ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            return get_audio_transcription(video_id)
            
    except Exception as e:
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return None

def get_audio_transcription(video_id):
    """éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œ"""
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            audio_path = os.path.join(temp_dir, f"temp_audio_{video_id}.mp4")
            
            st.write("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
            yt = YouTube(f'https://www.youtube.com/watch?v={video_id}')
            audio_stream = yt.streams.filter(only_audio=True).first()
            audio_stream.download(filename=audio_path)
            
            st.write("éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ä¸­...")
            model = whisper.load_model("base")
            
            st.write("éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œä¸­...")
            result = model.transcribe(audio_path)
            
            st.success("âœ… éŸ³å£°èªè­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return {
                'text': result["text"],
                'source': 'éŸ³å£°èªè­˜'
            }
            
    except Exception as e:
        st.error(f"éŸ³å£°èªè­˜ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return None

def analyze_text_content(text_data):
    """ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æã‚’æ™‚é–“æƒ…å ±ä»˜ãã§å®Ÿè¡Œ"""
    if not text_data:
        return
        
    text = text_data['text']
    normalized_text = normalize_text(text)
    issues = check_inappropriate_content(normalized_text)
    
    if issues:
        st.warning(f"{text_data['source']}ã‹ã‚‰ä»¥ä¸‹ã®æ³¨æ„ç‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š")
        
        # æ™‚é–“æƒ…å ±ä»˜ãã§å•é¡Œç®‡æ‰€ã‚’è¡¨ç¤º
        if 'segments' in text_data:
            for issue in issues:
                st.write(issue)
                # è©²å½“ç®‡æ‰€ã‚’æ¢ã—ã¦æ™‚é–“æƒ…å ±ã¨å…±ã«è¡¨ç¤º
                for segment in text_data['segments']:
                    normalized_segment = normalize_text(segment['text'])
                    if any(keyword in normalized_segment.lower() for keyword in get_keywords_from_issue(issue)):
                        st.write(f"â†’{segment['time']}ã€Œ{segment['text']}ã€")
        else:
            # éŸ³å£°èªè­˜ã®å ´åˆã¯æ™‚é–“æƒ…å ±ãªã—ã§è¡¨ç¤º
            for issue in issues:
                st.write(issue)
    else:
        st.success(f"{text_data['source']}ã‹ã‚‰ã¯å•é¡Œã¨ãªã‚‹è¡¨ç¾ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

def get_keywords_from_issue(issue):
    """å•é¡Œå ±å‘Šã‹ã‚‰æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    start = issue.find("'") + 1
    end = issue.find("'", start)
    if start != -1 and end != -1:
        return [issue[start:end]]
    return []

def analyze_video_frames(video_id):
    st.subheader("å‹•ç”»å†…ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ")

    try:
        st.write("å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
        with tempfile.TemporaryDirectory() as temp_dir:
            video_path = os.path.join(temp_dir, 'video.mp4')

            ydl_opts = {
                'format': 'best[height<=720]',
                'outtmpl': video_path,
            }

            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([f'https://www.youtube.com/watch?v={video_id}'])

            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))

            model = ResNet50(weights='imagenet')

            st.write("ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æã‚’é–‹å§‹...")
            progress_bar = st.progress(0)

            frames = []
            frame_count = 0
            sampling_rate = 5 * fps  # 5ç§’ã”ã¨ã«ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—

            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break

                if frame_count % sampling_rate == 0:
                    frames.append({
                        'frame': frame,
                        'time': frame_count // fps
                    })

                frame_count += 1
                progress_bar.progress(frame_count / total_frames)

            cap.release()

            st.success(f"å‹•ç”»åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆå–å¾—ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(frames)}ï¼‰")

            # å‹•ç”»å†ç”Ÿãƒãƒ¼ã®ä½œæˆ
            current_frame_index = st.slider("å†ç”Ÿä½ç½®", 0, len(frames) - 1, 0)
            current_frame = frames[current_frame_index]['frame']
            current_time = frames[current_frame_index]['time']

            # ç¾åœ¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¡¨ç¤º
            st.image(current_frame, use_container_width=True)
            st.write(f"å†ç”Ÿæ™‚é–“: {current_time}ç§’")

            # å±é™ºãªå€‹æ‰€ã®æ¤œå‡ºã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¡¨ç¤º
            pil_image = Image.fromarray(cv2.cvtColor(current_frame, cv2.COLOR_BGR2RGB))
            img = pil_image.resize((224, 224))
            x = image.img_to_array(img)
            x = np.expand_dims(x, axis=0)
            x = preprocess_input(x)

            preds = model.predict(x)
            predictions = decode_predictions(preds, top=5)[0]

            for _, label, confidence in predictions:
                if confidence > 0.2:
                    if 'weapon' in label or 'gun' in label:
                        st.warning(f"å±é™ºç‰©ã®å¯èƒ½æ€§: {label} ({confidence*100:.1f}%)")

    except Exception as e:
        st.error(f"å‹•ç”»åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")


def display_frame_results(frames_with_findings, video_path):
    cap = cv2.VideoCapture(video_path)
    for frame_data in frames_with_findings:
        frame_time = frame_data['time']
        frame_label = frame_data['label']
        confidence = frame_data['confidence']

        cap.set(cv2.CAP_PROP_POS_MSEC, frame_time * 1000)
        ret, frame = cap.read()
        if ret:
            st.image(frame, caption=f"Time: {frame_time}s, Label: {frame_label} ({confidence})", use_column_width=True)
    cap.release()

def main():
    st.title('YouTubeå‹•ç”»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒã‚§ãƒƒã‚«ãƒ¼')
    url = st.text_input('YouTube URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š')

    if url:
        video_id = extract_video_id(url)
        if video_id:
            try:
                video_response = youtube.videos().list(
                    part='snippet,contentDetails,status',
                    id=video_id
                ).execute()
                
                if video_response['items']:
                    video_data = video_response['items'][0]
                    
                    st.header('åŸºæœ¬æƒ…å ±')
                    st.write('ã‚¿ã‚¤ãƒˆãƒ«:', video_data['snippet']['title'])
                    st.write('ãƒãƒ£ãƒ³ãƒãƒ«å:', video_data['snippet']['channelTitle'])
                    st.write('æŠ•ç¨¿æ—¥:', video_data['snippet']['publishedAt'])
                    
                    st.header('ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹åˆ†æ')
                    analysis_results = analyze_video_content(video_data)
                    display_analysis_results(analysis_results)
                    
                    st.header('éŸ³å£°/å­—å¹•ã®åˆ†æ')
                    transcript_result = get_video_transcription(video_id)
                    
                    if transcript_result:
                        with st.expander("å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º"):
                            st.write(transcript_result['text'])
                        
                        analyze_text_content(transcript_result)

                    # å‹•ç”»å†…å®¹ã®è¦–è¦šåˆ†æã‚’å‘¼ã³å‡ºã™
                    st.header('å‹•ç”»å†…å®¹ã®è¦–è¦šåˆ†æ')
                    analyze_video_frames(video_id)
                    
                else:
                    st.error('å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ')
            
            except Exception as e:
                st.error(f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}')
        else:
            st.error('æœ‰åŠ¹ãªYouTube URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„')

if __name__ == '__main__':
    main()